{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "set_label = ['50-25','75-37','100-50','125-67','150-75']\n",
    "set_A_05 = ['1226_1700', '1226_1713', '1226_1727', '1226_1741', '1226_1756']\n",
    "set_B_02 = ['0104_2223', '0104_2309', '0104_2328', '0104_2343', '0105_0013']\n",
    "set_D_06 = ['0104_1949', '0104_2002', '0104_2016', '0104_2032', '0104_2056']\n",
    "\n",
    "subset = set_B_02\n",
    "set_name = './imagesets/B_S1_0.08.txt'\n",
    "# B:8.04% -> 1.1\n",
    "# D:9.15% -> 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(s1s2_path)\n",
    "# X = np.array([df['scan_enp'], df['bev_enp']]).T\n",
    "# y = np.array(df['s1_gt'])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "def evaluate_regression(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return mse, rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, scan_entropy_list, bev_entropy_list, pf_scalar_list, pf_vector_list, s1_gt =[], [], [], [], [], []\n",
    "empty_file = []\n",
    "sub_cnt = 0\n",
    "for time_dirc in subset:\n",
    "    sem_pt_path = '/home/newDisk/tool/carla_dataset_tool/raw_data/record_2024_'+time_dirc+'/vehicle.tesla.model3.master/velodyne_semantic/'\n",
    "    file_list = os.listdir(sem_pt_path)\n",
    "    file_list = [f for f in file_list if f.endswith('.txt')]\n",
    "    file_list.sort()\n",
    "\n",
    "    for file_name in file_list:\n",
    "        gt = 0\n",
    "        with open(sem_pt_path+file_name, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            last_line = lines[-1]\n",
    "            if len(lines) ==1:\n",
    "                empty_file.append(str(sub_cnt)+file_name[-9:-4])\n",
    "            else:\n",
    "                scores = last_line.split(' ')\n",
    "                s1_gt.append(scores[0])\n",
    "                scan_entropy_list.append(np.float64(scores[1]))\n",
    "                bev_entropy_list.append(np.float64(scores[2]))\n",
    "                pf_scalar_list.append(np.float64(scores[3]))\n",
    "                pf_vector_list.append(np.float64(scores[4].replace('\\n','')))\n",
    "                idx.append(str(sub_cnt)+file_name[-9:-4])\n",
    "\n",
    "    sub_cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4658"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scan_entropy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'idx':idx, 'scan_enp':scan_entropy_list, 'bev_enp':bev_entropy_list, 'pf_scalar':pf_scalar_list, 'pf_vector':pf_vector_list, 's1_gt':s1_gt})\n",
    "X = np.array([df['scan_enp'], df['bev_enp'],df['idx']]).T\n",
    "y = np.array(df['s1_gt'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "train_idx = X_train[:,-1]\n",
    "test_idx = X_test[:,-1]\n",
    "X_train = X_train[:,:-1]\n",
    "X_test = X_test[:,:-1]\n",
    "\n",
    "# sns.scatterplot(x=bev_entropy_list, y=s1_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931 3727 1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GBR x and y] test on train dataset\n",
      "mse:  2.1189078522767715\n",
      "rmse:  1.4556468844732817\n",
      "mae:  0.7903050667500368\n",
      "r2:  0.7627184542263621\n",
      "[GBR x and y] test on test dataset\n",
      "mse:  5.312919396041059\n",
      "rmse:  2.304977092302884\n",
      "mae:  1.4923082701282764\n",
      "r2:  0.3897560535700091\n",
      "0.08049369466058492\n",
      "accuracy:  0.813254628387443\n",
      "3427 3727\n"
     ]
    }
   ],
   "source": [
    "mertic = 1.1\n",
    "print(len(X_train), len(X_test), mertic)\n",
    "\n",
    "gbr = GradientBoostingRegressor(loss='huber',learning_rate=0.2,n_estimators=100, min_samples_leaf=10, max_depth=10, max_features=15, random_state=42)\n",
    "gbr.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred_train = gbr.predict(X_train)\n",
    "gbr_mse, gbr_rmse, gbr_mae, gbr_r2 = evaluate_regression(y_train, y_pred_train)\n",
    "print('[GBR x and y] test on train dataset')\n",
    "print('mse: ', gbr_mse)\n",
    "print('rmse: ', gbr_rmse)\n",
    "print('mae: ', gbr_mae)\n",
    "print('r2: ', gbr_r2)\n",
    "\n",
    "y_pred_test = gbr.predict(X_test)\n",
    "gbr_mse, gbr_rmse, gbr_mae, gbr_r2 = evaluate_regression(y_test, y_pred_test)\n",
    "print('[GBR x and y] test on test dataset')\n",
    "print('mse: ', gbr_mse)\n",
    "print('rmse: ', gbr_rmse)\n",
    "print('mae: ', gbr_mae)\n",
    "print('r2: ', gbr_r2)\n",
    "\n",
    "# calculate the accuary with metric\n",
    "y_pred2 = y_pred_test.copy()\n",
    "y_test2 = y_test.copy()\n",
    "y_pred2[y_pred2 <= mertic] = 0\n",
    "y_pred2[y_pred2 > mertic] = 1\n",
    "y_test2[np.float64(y_test2) <= mertic] = 0\n",
    "y_test2[np.float64(y_test2) > mertic] = 1\n",
    "accuracy = np.sum(y_pred2 == y_test2) / len(y_test2)\n",
    "\n",
    "print(len(y_pred2[y_pred2 == 0]) / len(y_pred2))\n",
    "\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "        \n",
    "# get the idx, where y_pred2 == 1 at the same place\n",
    "idx_test = test_idx[y_pred2 == 1]\n",
    "\n",
    "print(len(idx_test), len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_id = train_idx + test_idx, write to imageset\n",
    "train_list = train_idx.tolist() + idx_test.tolist()\n",
    "train_list.sort()\n",
    "\n",
    "test_list = []\n",
    "val_list = []\n",
    "test_spilt = 12\n",
    "val_spilt = 15\n",
    "\n",
    "# idx should not has element in empty_file\n",
    "for id in empty_file:\n",
    "    if id in train_list:\n",
    "        train_list.remove(id)\n",
    "\n",
    "train_path = './imagesets/B/train.txt'\n",
    "test_path = './imagesets/B/test.txt'\n",
    "val_path = './imagesets/B/val.txt'\n",
    "\n",
    "\n",
    "for i in range(len(train_list)):\n",
    "    if i % test_spilt == 0:\n",
    "        test_list.append(train_list[i])\n",
    "        train_list[i] = 'none'\n",
    "    if i % val_spilt == 0:\n",
    "        val_list.append(train_list[i])\n",
    "        train_list[i] = 'none'\n",
    "\n",
    "train_list = [x for x in train_list if x != 'none']\n",
    "val_list = [x for x in val_list if x != 'none']\n",
    "\n",
    "with open(train_path, 'w') as f:\n",
    "    for item in train_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(test_path, 'w') as f:\n",
    "    for item in test_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(val_path, 'w') as f:\n",
    "    for item in val_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './imagesets/D/train.txt'\n",
    "test_path = './imagesets/D/test.txt'\n",
    "val_path = './imagesets/D/val.txt'\n",
    "\n",
    "save_path = '/home/newDisk/tool/carla_dataset_tool/dataset/'\n",
    "\n",
    "# read imageset(train_path, test_path, val_path)\n",
    "train_set = set()\n",
    "test_set = set()\n",
    "val_set = set()\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        train_set.add(line.replace('\\n',''))\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        test_set.add(line.replace('\\n',''))\n",
    "\n",
    "with open(val_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        val_set.add(line.replace('\\n',''))\n",
    "\n",
    "set_A_05 = ['1226_1700', '1226_1713', '1226_1727', '1226_1741', '1226_1756']\n",
    "set_B_02 = ['0104_2223', '0104_2309', '0104_2328', '0104_2343', '0105_0013']\n",
    "set_D_06 = ['0104_1949', '0104_2002', '0104_2016', '0104_2032', '0104_2056']\n",
    "\n",
    "current_set = set_B_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import argparse\n",
    "\n",
    "# proj_path = '/home/newDisk/tool/carla_dataset_tool/'\n",
    "# dir_set = []\n",
    "\n",
    "# for i in range(len(current_set)):\n",
    "#     subset = current_set[i]\n",
    "#     dir = '/home/newDisk/tool/carla_dataset_tool/raw_data/record_2024_'+subset\n",
    "#     dir_set.append(dir)\n",
    "\n",
    "\n",
    "# # get inner frame id\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # copy lidar\n",
    "# target_path = {'train': proj_path+'dataset/training/velodyne/', 'test': proj_path+'dataset/testing/velodyne/'}\n",
    "# for key in target_path:\n",
    "#     if not os.path.exists(target_path[key]):\n",
    "#         os.makedirs(target_path[key])\n",
    "# source_path = []\n",
    "# for i in dir_set:\n",
    "#     source_path.append(i+'/vehicle.tesla.model3.master/velodyne/')\n",
    "\n",
    "# for train_frame in train_set:\n",
    "#     file_name = '00000' + train_frame[-5:]+'.bin'\n",
    "#     dir = source_path[int(train_frame[0])]\n",
    "#     file_path = dir+file_name\n",
    "#     shutil.copy(os.path.join(dir, file_name), os.path.join(target_path['train'], train_frame+'.bin'))\n",
    "\n",
    "# for test_frame in test_set:\n",
    "#     file_name = '00000' + test_frame[-5:]+'.bin'\n",
    "#     dir = source_path[int(test_frame[0])]\n",
    "#     file_path = dir+file_name\n",
    "#     shutil.copy(os.path.join(dir, file_name), os.path.join(target_path['test'], test_frame+'.bin'))\n",
    "\n",
    "# for val_frame in val_set:\n",
    "#     file_name = '00000' + val_frame[-5:]+'.bin'\n",
    "#     dir = source_path[int(val_frame[0])]\n",
    "#     file_path = dir+file_name\n",
    "#     shutil.copy(os.path.join(dir, file_name), os.path.join(target_path['train'], val_frame+'.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/newDisk/tool/carla_dataset_tool/raw_data/record_2024_0104_2002/vehicle.tesla.model3.master/image_2/0000020593.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/newDisk/tool/carla_dataset_tool/ada_exp/gen_data/S1_spilt.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/newDisk/tool/carla_dataset_tool/ada_exp/gen_data/S1_spilt.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m source_path[\u001b[39mint\u001b[39m(train_frame[\u001b[39m0\u001b[39m])]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/newDisk/tool/carla_dataset_tool/ada_exp/gen_data/S1_spilt.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     file_path \u001b[39m=\u001b[39m \u001b[39mdir\u001b[39m\u001b[39m+\u001b[39mfile_name\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/newDisk/tool/carla_dataset_tool/ada_exp/gen_data/S1_spilt.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopy(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mdir\u001b[39;49m, file_name), os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(target_path[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], train_frame\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.png\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/newDisk/tool/carla_dataset_tool/ada_exp/gen_data/S1_spilt.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m test_frame \u001b[39min\u001b[39;00m test_set:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/newDisk/tool/carla_dataset_tool/ada_exp/gen_data/S1_spilt.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     file_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m00000\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m test_frame[\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m:]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/carla15/lib/python3.8/shutil.py:418\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(dst):\n\u001b[1;32m    417\u001b[0m     dst \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dst, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(src))\n\u001b[0;32m--> 418\u001b[0m copyfile(src, dst, follow_symlinks\u001b[39m=\u001b[39;49mfollow_symlinks)\n\u001b[1;32m    419\u001b[0m copymode(src, dst, follow_symlinks\u001b[39m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    420\u001b[0m \u001b[39mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m~/anaconda3/envs/carla15/lib/python3.8/shutil.py:264\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    262\u001b[0m     os\u001b[39m.\u001b[39msymlink(os\u001b[39m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(src, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fsrc, \u001b[39mopen\u001b[39m(dst, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fdst:\n\u001b[1;32m    265\u001b[0m         \u001b[39m# macOS\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[39mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[1;32m    267\u001b[0m             \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/newDisk/tool/carla_dataset_tool/raw_data/record_2024_0104_2002/vehicle.tesla.model3.master/image_2/0000020593.png'"
     ]
    }
   ],
   "source": [
    "# # copy image\n",
    "# target_path = {'train': proj_path+'dataset/training/image_2/', 'test': proj_path+'dataset/testing/image_2/'}\n",
    "# for key in target_path:\n",
    "#     if not os.path.exists(target_path[key]):\n",
    "#         os.makedirs(target_path[key])\n",
    "# source_path = []\n",
    "# for i in dir_set:\n",
    "#     source_path.append(i+'/vehicle.tesla.model3.master/image_2/')\n",
    "# for train_frame in train_set:\n",
    "#     file_name = '00000' + train_frame[-5:]+'.png'\n",
    "#     dir = source_path[int(train_frame[0])]\n",
    "#     file_path = dir+file_name\n",
    "#     shutil.copy(os.path.join(dir, file_name), os.path.join(target_path['train'], train_frame+'.png'))\n",
    "# for test_frame in test_set:\n",
    "#     file_name = '00000' + test_frame[-5:]+'.png'\n",
    "#     dir = source_path[int(test_frame[0])]\n",
    "#     file_path = dir+file_name\n",
    "#     shutil.copy(os.path.join(dir, file_name), os.path.join(target_path['test'], test_frame+'.png'))\n",
    "# for val_frame in val_set:\n",
    "#     file_name = '00000' + val_frame[-5:]+'.png'\n",
    "#     dir = source_path[int(val_frame[0])]\n",
    "#     file_path = dir+file_name\n",
    "#     shutil.copy(os.path.join(dir, file_name), os.path.join(target_path['train'], val_frame+'.png'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
